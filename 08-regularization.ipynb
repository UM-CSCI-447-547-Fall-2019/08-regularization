{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 8: Regularization\n",
    "\n",
    "## Reading: Bishop 5.5\n",
    "\n",
    "## 1 Motivating regularization\n",
    "\n",
    "“YEAH, BUT YOUR SCIENTISTS WERE SO PREOCCUPIED WITH WHETHER OR NOT THEY COULD THAT THEY DIDN’T STOP TO THINK IF THEY SHOULD.” -- Dr. Ian Malcolm\n",
    "\n",
    "<img src=\"images/goldblum.jpg\">\n",
    "\n",
    "\n",
    "We now have the power to create functions (namely neural networks) that have the power to approximate any other function, given a sufficient number of parameters.  However, as we learned when we were fitting polynomials to curves all the way back at the start of the class, unlimited model complexity is a path fraught with peril.  Recall that if we have a simple dataset like this one:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (9,9)\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Create constantly-spaced x-values\n",
    "x = np.linspace(0,1,11)\n",
    "\n",
    "# Create a linear function of $x$ with slope 1, intercept 1, and normally distributed error with sd=1\n",
    "y = x + np.random.randn(len(x))*0.1 + 1.0\n",
    "y_test = x + np.random.randn(len(x))*0.1 + 1.0\n",
    "plt.plot(x,y,'k.')\n",
    "plt.plot(x,y_test,'r*')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fit arbitrarily complex models such that we hit the training data exactly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y,'k.')\n",
    "x_smooth = np.linspace(0,1,101)\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "for d in range(1,13,2):\n",
    "    X = np.vander(x,d,increasing=True)\n",
    "    w = np.linalg.solve(X.T @ X,X.T@y)\n",
    "    y_pred = X@w\n",
    "    plt.plot(x_smooth,np.vander(x_smooth,d,increasing=True)@w)\n",
    "    training_error = 1./len(y)*np.sum((y_pred-y)**2)\n",
    "    test_error = 1./len(y_test)*np.sum((y_pred-y_test)**2)\n",
    "    train_errors.append(training_error)\n",
    "    test_errors.append(test_error)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we then plot the resulting training set and test errors, we see this typical pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_errors,label='Training error')\n",
    "plt.plot(test_errors,label='Test error')\n",
    "plt.legend()\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course the same thing can happen in neural networks.  In the case of linear regression, we made the simple choice to just limit our model complexity.  This is certainly possible with neural nets.  However, it's a little bit more challenging to decide just exactly *where* overfitting is coming from, and to tailor the network in response.  Instead, we introduce a technique that we will refer to as *regularization*.  Regularization is broadly understood to be a technique that trades training set accuracy for test set accuracy, and there are a multitude of flavors, a few of which we explore here.  \n",
    "\n",
    "## 2 L2 Regularization\n",
    "The most common idea for regularizing has been around for a very long time, and it results from the observation that in most regression problems, large weights tend to correspond to overfitting the data.  As such, we can make an effort to reduce overfitting by explicitly penalizing large parameters in the cost function.  In particular, we can simply add the following term:\n",
    "$$\n",
    "\\mathcal{L}' = \\underbrace{\\mathcal{L}}_{\\text{Sum Square Error}} + \\underbrace{\\frac{\\gamma}{2}\\; \\mathbf{w}^T \\mathbf{w}}_{L_2-\\text{norm of }\\mathbf{w}}.\n",
    "$$\n",
    "It is straightforward to take the derivative of this augmented cost function with respect to the parameters, and setting them to zero yields an augmented set of normal equations:\n",
    "$$\n",
    "(X^T X + \\gamma \\mathcal{I}) \\mathbf{w} = X^T y,\n",
    "$$\n",
    "where $\\mathcal{I}$ is an appropriately sized identity matrix.  **What does the parameter $\\gamma$ do**?\n",
    "\n",
    "We can easily implement this for a high-dimensional problem, and explore what happens to our model fit as we adjust $\\gamma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y,'k.')\n",
    "x_smooth = np.linspace(0,1,101)\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "d = 13\n",
    "gammas = np.logspace(-7,1,12)\n",
    "for gamma in gammas:\n",
    "    X = np.vander(x,d,increasing=True)\n",
    "    identity = np.eye(X.shape[1])\n",
    "    identity[0,0] = 0\n",
    "    w = np.linalg.solve(X.T @ X + gamma*identity,X.T@y)\n",
    "    y_pred = X@w\n",
    "    plt.plot(x_smooth,np.vander(x_smooth,d,increasing=True)@w)\n",
    "    training_error = 1./len(y)*np.sum((y_pred-y)**2)\n",
    "    test_error = 1./len(y_test)*np.sum((y_pred-y_test)**2)\n",
    "    train_errors.append(training_error)\n",
    "    test_errors.append(test_error)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we can look at the test and training accuracies together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogx(gammas,train_errors,label='Training error')\n",
    "plt.semilogx(gammas,test_errors,label='Test error')\n",
    "plt.legend()\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As similar picture emerges (although flipped around the x-axis, because a larger $\\gamma$ corresponds to a simpler model.  \n",
    "\n",
    "## L2 Regularization for neural networks\n",
    "\n",
    "As it turns out, this is simple to apply to neural networks as well.  To illustrate its effect, let's synthesize a very noisy dataset to be used for a classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X,y = make_moons(n_samples=300,noise=0.5)\n",
    "\n",
    "X,X_test,y,y_test = train_test_split(X,y)\n",
    "X0,y0 = X.copy(),y.copy()\n",
    "X0_test,y0_test = X_test.copy(),y_test.copy()\n",
    "\n",
    "plt.scatter(X[:,0],X[:,1],c=y)\n",
    "plt.scatter(X_test[:,0],X_test[:,1],c=y_test,marker='x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a clear pattern here, but it is quite noisy.  Let's see if we can fit a very flexible neural network to this dataset using pytorch.  We'll first go through our ritual of converting the data into the appropriate type and location, and then create a DataLoader for use with batch gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "X = torch.from_numpy(X)\n",
    "X_test = torch.from_numpy(X_test)\n",
    "y = torch.from_numpy(y)\n",
    "y_test = torch.from_numpy(y_test)\n",
    "\n",
    "\n",
    "X = X.to(torch.float32)\n",
    "X_test = X_test.to(torch.float32)\n",
    "y = y.to(torch.long)\n",
    "y_test = y_test.to(torch.long)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "X = X.to(device)\n",
    "X_test = X_test.to(device)\n",
    "y = y.to(device)\n",
    "y_test = y_test.to(device)\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "training_data = TensorDataset(X,y)\n",
    "test_data = TensorDataset(X_test,y_test)\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = torch.utils.data.DataLoader(dataset=training_data,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "batch_size = 256\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define our neural network.  It'll be a simple affair with a single hidden layer, but plenty of nodes to ensure a flexible function.  Something like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        This method is where you'll want to instantiate parameters.\n",
    "        we do this by creating two linear transformation functions, l1 and l2, which \n",
    "        have encoded in it both the weight matrices W_1 and W_2, and the bias vectors\n",
    "        \"\"\"\n",
    "        super(Net,self).__init__()\n",
    "        self.l1 = nn.Linear(2,2048) # Transform from input to hidden layer\n",
    "        self.l2 = nn.Linear(2048,2)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        This method runs the feedforward neural network.  It takes a tensor of size m x 784,\n",
    "        applies a linear transformation, applies a sigmoidal activation, applies the second linear transform \n",
    "        and outputs the logits.\n",
    "        \"\"\"\n",
    "        a1 = self.l1(x)\n",
    "        z1 = torch.relu(a1)\n",
    "        \n",
    "        a2 = self.l2(z1)\n",
    "        return a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "epochs = 3000\n",
    "# Loop over the data\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    # Loop over each subset of data\n",
    "    for d,t in train_loader:\n",
    "\n",
    "        # Zero out the optimizer's gradient buffer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Make a prediction based on the model\n",
    "        outputs = model(d)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs,t)      \n",
    "\n",
    "        # Use backpropagation to compute the derivative of the loss with respect to the parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Use the derivative information to update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "    model.eval()\n",
    "    # After each epoch, compute the test set accuracy\n",
    "    total=0.\n",
    "    correct=0.\n",
    "    # Loop over all the test examples and accumulate the number of correct results in each batch\n",
    "    for d,t in test_loader:\n",
    "        outputs = model(d)\n",
    "        _, predicted = torch.max(outputs.data,1)\n",
    "        total += float(t.size(0))\n",
    "        correct += float((predicted==t).sum())\n",
    "    total_train = 0\n",
    "    correct_train = 0\n",
    "    for d,t in train_loader:\n",
    "        outputs = model(d)\n",
    "        _, predicted = torch.max(outputs.data,1)\n",
    "        total_train += float(t.size(0))\n",
    "        correct_train += float((predicted==t).sum())\n",
    "        \n",
    "    # Print the epoch, the training loss, and the test set accuracy.\n",
    "    if epoch%10==0:\n",
    "        print(epoch,loss.item(),100.*correct_train/total_train,100.*correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The above case exhibits the classic symptoms of overfitting.  How do you know?**\n",
    "\n",
    "This neural network only has two features, so we can easily visualize its predictions as a grid. Let's see what the decision boundary is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0grid,X1grid = np.meshgrid(np.linspace(X[:,0].cpu().min(),X[:,0].cpu().max(),101),np.linspace(X[:,1].cpu().min(),X[:,1].cpu().max(),101))\n",
    "\n",
    "X_grid = np.vstack((X0grid.ravel(),X1grid.ravel())).T\n",
    "X_grid = torch.from_numpy(X_grid)\n",
    "X_grid = X_grid.to(torch.float32)\n",
    "X_grid = X_grid.cuda()\n",
    "\n",
    "t = model(X_grid)\n",
    "out = F.softmax(t,dim=1)\n",
    "plt.contourf(X0grid,X1grid,out.cpu().detach().numpy()[:,1].reshape((101,101)),alpha=0.5)\n",
    "plt.scatter(X0[:,0],X0[:,1],c=y0)\n",
    "plt.scatter(X0_test[:,0],X0_test[:,1],c=y0_test,marker='x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, the test set accuracy isn't so good because the model is learning a really crazy mapping from the features to the class probability!  We can allay this issue using L2 regularization.  How do we implement this?  As it turns out, it's not so bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "gamma = 1.0  ### HERE'S THE REGULARIZATION PARAMETER!\n",
    "\n",
    "epochs = 3000\n",
    "# Loop over the data\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    # Loop over each subset of data\n",
    "    for d,t in train_loader:\n",
    "\n",
    "        # Zero out the optimizer's gradient buffer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Make a prediction based on the model\n",
    "        outputs = model(d)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs,t)\n",
    "        \n",
    "        ### HERE'S WHERE WE ADD REGULARIZATION!\n",
    "        for p in model.parameters():\n",
    "            # Loop over all the model parameters\n",
    "            if p.dim()>1:\n",
    "                # Loop over all the weight matrices (but not the biases)\n",
    "                loss += gamma/(2*d.shape[0])*(p**2).sum()\n",
    "        \n",
    "\n",
    "        # Use backpropagation to compute the derivative of the loss with respect to the parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Use the derivative information to update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "    model.eval()\n",
    "    # After each epoch, compute the test set accuracy\n",
    "    total=0.\n",
    "    correct=0.\n",
    "    # Loop over all the test examples and accumulate the number of correct results in each batch\n",
    "    for d,t in test_loader:\n",
    "        outputs = model(d)\n",
    "        _, predicted = torch.max(outputs.data,1)\n",
    "        total += float(t.size(0))\n",
    "        correct += float((predicted==t).sum())\n",
    "    total_train = 0\n",
    "    correct_train = 0\n",
    "    for d,t in train_loader:\n",
    "        outputs = model(d)\n",
    "        _, predicted = torch.max(outputs.data,1)\n",
    "        total_train += float(t.size(0))\n",
    "        correct_train += float((predicted==t).sum())\n",
    "        \n",
    "    # Print the epoch, the training loss, and the test set accuracy.\n",
    "    if epoch%10==0:\n",
    "        print(epoch,loss.item(),100.*correct_train/total_train,100.*correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0grid,X1grid = np.meshgrid(np.linspace(X[:,0].cpu().min(),X[:,0].cpu().max(),101),np.linspace(X[:,1].cpu().min(),X[:,1].cpu().max(),101))\n",
    "\n",
    "X_grid = np.vstack((X0grid.ravel(),X1grid.ravel())).T\n",
    "X_grid = torch.from_numpy(X_grid)\n",
    "X_grid = X_grid.to(torch.float32)\n",
    "X_grid = X_grid.cuda()\n",
    "\n",
    "t = model(X_grid)\n",
    "out = F.softmax(t,dim=1)\n",
    "plt.contourf(X0grid,X1grid,out.cpu().detach().numpy()[:,1].reshape((101,101)),alpha=0.5)\n",
    "plt.scatter(X0[:,0],X0[:,1],c=y0)\n",
    "plt.scatter(X0_test[:,0],X0_test[:,1],c=y0_test,marker='x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to have helped alot.  What happens if we adjust $\\gamma$?  **Explore this value on your own, and determine what you think a sensible value is**.\n",
    "\n",
    "## L2 Regularization for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# In order to run this in class, we're going to reduce the dataset by a factor of 5\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, cache=True)\n",
    "X/=255.\n",
    "y = y.astype(int)\n",
    "X,X_test,y,y_test = train_test_split(X,y)\n",
    "X = X\n",
    "y = y\n",
    "\n",
    "X = torch.from_numpy(X)\n",
    "X_test = torch.from_numpy(X_test)\n",
    "y = torch.from_numpy(y)\n",
    "y_test = torch.from_numpy(y_test)\n",
    "\n",
    "X = X.to(torch.float32)\n",
    "X_test = X_test.to(torch.float32)\n",
    "y = y.to(torch.long)\n",
    "y_test = y_test.to(torch.long)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "X = X.to(device)\n",
    "X_test = X_test.to(device)\n",
    "y = y.to(device)\n",
    "y_test = y_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "training_data = TensorDataset(X,y)\n",
    "test_data = TensorDataset(X_test,y_test)\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = torch.utils.data.DataLoader(dataset=training_data,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "batch_size = 256\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        This method is where you'll want to instantiate parameters.\n",
    "        we do this by creating two linear transformation functions, l1 and l2, which \n",
    "        have encoded in it both the weight matrices W_1 and W_2, and the bias vectors\n",
    "        \"\"\"\n",
    "        super(Net,self).__init__()\n",
    "        self.l1 = nn.Linear(784,256) # Transform from input to hidden layer\n",
    "        self.l2 = nn.Linear(256,256)\n",
    "        self.l3 = nn.Linear(256,10)\n",
    "        \n",
    "        self.dropout_1 = nn.Dropout(p=0.3)\n",
    "        self.dropout_2 = nn.Dropout(p=0.3)\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        This method runs the feedforward neural network.  It takes a tensor of size m x 784,\n",
    "        applies a linear transformation, applies a sigmoidal activation, applies the second linear transform \n",
    "        and outputs the logits.\n",
    "        \"\"\"\n",
    "        a1 = self.l1(x)\n",
    "        z1 = torch.relu(a1)\n",
    "        z1d = self.dropout_1(z1)\n",
    "        \n",
    "        a2 = self.l2(z1d)\n",
    "        z2 = torch.relu(a2)\n",
    "        z2d = self.dropout_2(z2)\n",
    "       # \n",
    "        a3 = self.l3(z2d)\n",
    "\n",
    "        return a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "optimizer = torch.optim.RMSprop(model.parameters(),lr=1e-3,momentum=0.9)\n",
    "gamma = 0\n",
    "\n",
    "epochs = 200\n",
    "# Loop over the data\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    # Loop over each subset of data\n",
    "    for d,t in train_loader:\n",
    "\n",
    "        # Zero out the optimizer's gradient buffer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Make a prediction based on the model\n",
    "        outputs = model(d)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs,t)\n",
    "        for p in model.parameters():\n",
    "            if p.dim()>1:\n",
    "                loss += gamma/(2*d.shape[0])*(p**2).sum()\n",
    "        \n",
    "\n",
    "        # Use backpropagation to compute the derivative of the loss with respect to the parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Use the derivative information to update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "    model.eval()\n",
    "    # After each epoch, compute the test set accuracy\n",
    "    total=0.\n",
    "    correct=0.\n",
    "    # Loop over all the test examples and accumulate the number of correct results in each batch\n",
    "    for d,t in test_loader:\n",
    "        outputs = model(d)\n",
    "        _, predicted = torch.max(outputs.data,1)\n",
    "        total += float(t.size(0))\n",
    "        correct += float((predicted==t).sum())\n",
    "    total_train = 0\n",
    "    correct_train = 0\n",
    "    for d,t in train_loader:\n",
    "        outputs = model(d)\n",
    "        _, predicted = torch.max(outputs.data,1)\n",
    "        total_train += float(t.size(0))\n",
    "        correct_train += float((predicted==t).sum())\n",
    "        \n",
    "    # Print the epoch, the training loss, and the test set accuracy.\n",
    "    print(epoch,loss.item(),100.*correct_train/total_train,100.*correct/total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
