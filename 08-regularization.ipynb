{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 8: Regularization\n",
    "\n",
    "## Reading: Bishop 5.5\n",
    "\n",
    "## 1 Motivating regularization\n",
    "\n",
    "“YEAH, BUT YOUR SCIENTISTS WERE SO PREOCCUPIED WITH WHETHER OR NOT THEY COULD THAT THEY DIDN’T STOP TO THINK IF THEY SHOULD.” -- Dr. Ian Malcolm\n",
    "\n",
    "<img src=\"images/goldblum.jpg\">\n",
    "\n",
    "\n",
    "We now have the power to create functions (namely neural networks) that have the power to approximate any other function, given a sufficient number of parameters.  However, as we learned when we were fitting polynomials to curves all the way back at the start of the class, unlimited model complexity is a path fraught with peril.  Recall that if we have a simple dataset like this one:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (9,9)\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Create constantly-spaced x-values\n",
    "x = np.linspace(0,1,11)\n",
    "\n",
    "# Create a linear function of $x$ with slope 1, intercept 1, and normally distributed error with sd=1\n",
    "y = x + np.random.randn(len(x))*0.1 + 1.0\n",
    "y_test = x + np.random.randn(len(x))*0.1 + 1.0\n",
    "plt.plot(x,y,'k.')\n",
    "plt.plot(x,y_test,'r*')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fit arbitrarily complex models such that we hit the training data exactly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y,'k.')\n",
    "x_smooth = np.linspace(0,1,101)\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "degrees = range(1,13,2)\n",
    "for d in degrees:\n",
    "    X = np.vander(x,d,increasing=True)\n",
    "    w = np.linalg.solve(X.T @ X,X.T@y)\n",
    "    y_pred = X@w\n",
    "    plt.plot(x_smooth,np.vander(x_smooth,d,increasing=True)@w)\n",
    "    training_error = 1./len(y)*np.sum((y_pred-y)**2)\n",
    "    test_error = 1./len(y_test)*np.sum((y_pred-y_test)**2)\n",
    "    train_errors.append(training_error)\n",
    "    test_errors.append(test_error)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we then plot the resulting training set and test errors as a function of the number of degrees of freedom, we see this typical pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(degrees,train_errors,label='Training error')\n",
    "plt.plot(degrees,test_errors,label='Test error')\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('Objective function value')\n",
    "plt.legend()\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training error decreases with increased model complexity, while the test error initially decreases, then begins to *increase* as the model becomes more complex.\n",
    "\n",
    "Of course this very same thing can happen in neural networks (linear regression is, after all, a very simple version of a multilayer perceptron with no hidden layers and the identity as an activation function).  In the case of linear regression, we made the simple choice to just limit our model complexity.  This is certainly possible with neural nets, by limiting the number of hidden layers and nodes.  However, it's a little bit more challenging to decide just exactly *where* overfitting is coming from, and to tailor the network in response.  Instead, we introduce a technique that we will refer to as *regularization*.  Regularization is broadly understood to be a technique that trades training set accuracy for test set accuracy, and there are a multitude of flavors: we'll explore a few of them here.  \n",
    "\n",
    "## 2 $L_2$ Regularization\n",
    "The most common idea for regularizing has been around for a very long time, and it results from the observation that in most regression problems, large weights tend to correspond to overfitting the data.  As such, we can make an effort to reduce overfitting by explicitly penalizing large parameters in the cost function.  In particular, we can simply add the following term:\n",
    "$$\n",
    "\\mathcal{L}' = \\underbrace{\\mathcal{L}}_{\\text{Sum Square Error}} + \\frac{\\gamma}{2}\\; \\sum_{l=1}^{L} \\|W^{(l)}\\|_{2}^2,\n",
    "$$\n",
    "where \n",
    "$$\n",
    "\\|W^{(l)}\\|_2^2 = \\sum_{i} \\sum_{j} (w_{ij}^{(l)})^2\n",
    "$$\n",
    "is the square of the *Frobenius* norm, which generalizes the normal $L_2$ norm (aka Euclidean distance) to matrices.\n",
    "\n",
    "## IC8A\n",
    "$L_2$ regularization has many names including ridge regression and Tikhonov regularization.  However, in the world of machine learning, it is often called **weight decay**.  Compute the derivative of $\\frac{\\gamma}{2}\\; \\sum_{l=1}^L \\|W^{(l)}\\|_2^2$ with respect to some arbitrary weight $w_{ij}^{(l)}$ (ignore the misfit component of the cost function for now), and determine the resulting gradient descent update formula, e.g.\n",
    "$$\n",
    "w_{ij}^{(l)} \\leftarrow w_{ij}^{(l)} - \\ldots\n",
    "$$\n",
    "**Why is it called weight decay?**\n",
    "\n",
    "## 2a $L_2$ Regularization for linear regression\n",
    "The result derived above is general, but when applied to the linear regression problem we can write down the closed form solution for the optimal parameters as\n",
    "$$\n",
    "(X^T X + \\gamma \\mathcal{I}) \\mathbf{w} = X^T y,\n",
    "$$\n",
    "where $\\mathcal{I}$ is an appropriately sized identity matrix (The additive term along the matrix diagonal gives rise to the name *ridge regression*).  **What does the parameter $\\gamma$ do**?\n",
    "\n",
    "We can easily implement this for a high-dimensional problem, and explore what happens to our model fit as we adjust $\\gamma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y,'k.')\n",
    "x_smooth = np.linspace(0,1,101)\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "d = 13\n",
    "gammas = np.logspace(-7,1,12)\n",
    "for gamma in gammas:\n",
    "    X = np.vander(x,d,increasing=True)\n",
    "    identity = np.eye(X.shape[1])\n",
    "    identity[0,0] = 0 # Why do I do this?\n",
    "    w = np.linalg.solve(X.T @ X + gamma*identity,X.T@y)\n",
    "    y_pred = X@w\n",
    "    plt.plot(x_smooth,np.vander(x_smooth,d,increasing=True)@w)\n",
    "    training_error = 1./len(y)*np.sum((y_pred-y)**2)\n",
    "    test_error = 1./len(y_test)*np.sum((y_pred-y_test)**2)\n",
    "    train_errors.append(training_error)\n",
    "    test_errors.append(test_error)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we can look at the test and training accuracies together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogx(gammas,train_errors,label='Training error')\n",
    "plt.semilogx(gammas,test_errors,label='Test error')\n",
    "plt.legend()\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As similar picture emerges (although flipped around the x-axis, because a larger $\\gamma$ corresponds to a simpler model.  \n",
    "\n",
    "## 2b $L_2$ Regularization for neural networks\n",
    "\n",
    "As it turns out, this is simple to apply to neural networks as well.  To illustrate its effect, let's synthesize a very noisy dataset to be used for a classification problem.  This is not dissimilar to the iris data set, in the sense that there are classes that overlap one another in their features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X,y = make_moons(n_samples=300,noise=0.4)\n",
    "\n",
    "X,X_test,y,y_test = train_test_split(X,y)\n",
    "X0,y0 = X.copy(),y.copy()\n",
    "X0_test,y0_test = X_test.copy(),y_test.copy()\n",
    "\n",
    "plt.scatter(X[:,0],X[:,1],c=y)\n",
    "plt.scatter(X_test[:,0],X_test[:,1],c=y_test,marker='x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a clear pattern here, but it is quite noisy.  Let's see if we can fit a very flexible neural network to this dataset using pytorch.  We'll first go through our ritual of converting the data into the appropriate type and location, and then create a DataLoader for use with batch gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "X = torch.from_numpy(X)\n",
    "X_test = torch.from_numpy(X_test)\n",
    "y = torch.from_numpy(y)\n",
    "y_test = torch.from_numpy(y_test)\n",
    "\n",
    "\n",
    "X = X.to(torch.float32)\n",
    "X_test = X_test.to(torch.float32)\n",
    "y = y.to(torch.long)\n",
    "y_test = y_test.to(torch.long)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "X = X.to(device)\n",
    "X_test = X_test.to(device)\n",
    "y = y.to(device)\n",
    "y_test = y_test.to(device)\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "training_data = TensorDataset(X,y)\n",
    "test_data = TensorDataset(X_test,y_test)\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = torch.utils.data.DataLoader(dataset=training_data,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "batch_size = 256\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define our neural network.  It'll be a simple affair with a single hidden layer, but plenty of nodes to ensure a flexible function.  Something like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        This method is where you'll want to instantiate parameters.\n",
    "        we do this by creating two linear transformation functions, l1 and l2, which \n",
    "        have encoded in it both the weight matrices W_1 and W_2, and the bias vectors\n",
    "        \"\"\"\n",
    "        super(Net,self).__init__()\n",
    "        self.l1 = nn.Linear(2,2048) # Transform from input to hidden layer\n",
    "        self.l2 = nn.Linear(2048,2)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        This method runs the feedforward neural network.  It takes a tensor of size m x 784,\n",
    "        applies a linear transformation, applies a sigmoidal activation, applies the second linear transform \n",
    "        and outputs the logits.\n",
    "        \"\"\"\n",
    "        a1 = self.l1(x)\n",
    "        z1 = torch.relu(a1)\n",
    "        \n",
    "        a2 = self.l2(z1)\n",
    "        return a2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can optimize.  Let's pay attention to the training and test set accuracy as we optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "epochs = 3000\n",
    "# Loop over the data\n",
    "\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    # Loop over each subset of data\n",
    "    for d,t in train_loader:\n",
    "\n",
    "        # Zero out the optimizer's gradient buffer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Make a prediction based on the model\n",
    "        outputs = model(d)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs,t)      \n",
    "\n",
    "        # Use backpropagation to compute the derivative of the loss with respect to the parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Use the derivative information to update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "    model.eval()\n",
    "    # After each epoch, compute the test set accuracy\n",
    "    total=0.\n",
    "    correct=0.\n",
    "    # Loop over all the test examples and accumulate the number of correct results in each batch\n",
    "    for d,t in test_loader:\n",
    "        outputs = model(d)\n",
    "        _, predicted = torch.max(outputs.data,1)\n",
    "        total += float(t.size(0))\n",
    "        correct += float((predicted==t).sum())\n",
    "    total_train = 0\n",
    "    correct_train = 0\n",
    "    for d,t in train_loader:\n",
    "        outputs = model(d)\n",
    "        _, predicted = torch.max(outputs.data,1)\n",
    "        total_train += float(t.size(0))\n",
    "        correct_train += float((predicted==t).sum())\n",
    "        \n",
    "    # Print the epoch, the training loss, and the test set accuracy.\n",
    "    train_accs.append(100.*correct_train/total_train)\n",
    "    test_accs.append(100.*correct/total)\n",
    "    if epoch%10==0:\n",
    "        print(epoch,loss.item(),train_accs[-1],test_accs[-1])\n",
    "plt.plot(train_accs,label='Training accuracy')\n",
    "plt.plot(test_accs,label='Test accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The above case exhibits the classic symptoms of overfitting.  How do you know?  Based on the plot above, can you identify an alternative method of regularization?**\n",
    "\n",
    "This neural network only has two features, so we can easily visualize its predictions as a grid. Let's see what the decision boundary is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0grid,X1grid = np.meshgrid(np.linspace(X[:,0].cpu().min(),X[:,0].cpu().max(),101),np.linspace(X[:,1].cpu().min(),X[:,1].cpu().max(),101))\n",
    "\n",
    "X_grid = np.vstack((X0grid.ravel(),X1grid.ravel())).T\n",
    "X_grid = torch.from_numpy(X_grid)\n",
    "X_grid = X_grid.to(torch.float32)\n",
    "X_grid = X_grid.cuda()\n",
    "\n",
    "t = model(X_grid)\n",
    "out = F.softmax(t,dim=1)\n",
    "plt.contourf(X0grid,X1grid,out.cpu().detach().numpy()[:,1].reshape((101,101)),alpha=0.5)\n",
    "plt.scatter(X0[:,0],X0[:,1],c=y0,label='Training Data')\n",
    "plt.scatter(X0_test[:,0],X0_test[:,1],c=y0_test,marker='x',label='Test Data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What's the problem here?**\n",
    "\n",
    "We can allay this issue using $L_2$ regularization.  How do we implement this?  As it turns out, it's not so bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "gamma = 0.1  ### HERE'S THE REGULARIZATION PARAMETER!\n",
    "\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "epochs = 3000\n",
    "# Loop over the data\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    # Loop over each subset of data\n",
    "    for d,t in train_loader:\n",
    "\n",
    "        # Zero out the optimizer's gradient buffer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Make a prediction based on the model\n",
    "        outputs = model(d)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs,t)\n",
    "        \n",
    "        ### HERE'S WHERE WE ADD REGULARIZATION!\n",
    "        for W in model.parameters():\n",
    "            # Loop over all the model parameters\n",
    "            if W.dim()>1:\n",
    "                # Loop over all the weight matrices (but not the biases)\n",
    "                loss += gamma/(2*d.shape[0])*(W**2).sum()\n",
    "        \n",
    "\n",
    "        # Use backpropagation to compute the derivative of the loss with respect to the parameters\n",
    "        # NOTE THAT THIS NOW INCLUDES A DIRECT DEPENDENCY ON THE PARAMETERS DUE TO THE REGULARIZATION TERM\n",
    "        loss.backward()\n",
    "        \n",
    "        # Use the derivative information to update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "    model.eval()\n",
    "    # After each epoch, compute the test set accuracy\n",
    "    total=0.\n",
    "    correct=0.\n",
    "    # Loop over all the test examples and accumulate the number of correct results in each batch\n",
    "    for d,t in test_loader:\n",
    "        outputs = model(d)\n",
    "        _, predicted = torch.max(outputs.data,1)\n",
    "        total += float(t.size(0))\n",
    "        correct += float((predicted==t).sum())\n",
    "    total_train = 0\n",
    "    correct_train = 0\n",
    "    for d,t in train_loader:\n",
    "        outputs = model(d)\n",
    "        _, predicted = torch.max(outputs.data,1)\n",
    "        total_train += float(t.size(0))\n",
    "        correct_train += float((predicted==t).sum())\n",
    "        \n",
    "    # Print the epoch, the training loss, and the test set accuracy.\n",
    "    train_accs.append(100.*correct_train/total_train)\n",
    "    test_accs.append(100.*correct/total)\n",
    "    if epoch%100==0:\n",
    "        print(epoch,loss.item(),train_accs[-1],test_accs[-1])\n",
    "        \n",
    "plt.plot(train_accs,label='Training accuracy')\n",
    "plt.plot(test_accs,label='Test accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Does adding this regularization improve the overfitting issues?  How do you know?**\n",
    "\n",
    "**Critically, what happens if we increase the regularization parameter too far?**\n",
    "\n",
    "Let's print the resulting decision surface below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0grid,X1grid = np.meshgrid(np.linspace(X[:,0].cpu().min(),X[:,0].cpu().max(),101),np.linspace(X[:,1].cpu().min(),X[:,1].cpu().max(),101))\n",
    "\n",
    "X_grid = np.vstack((X0grid.ravel(),X1grid.ravel())).T\n",
    "X_grid = torch.from_numpy(X_grid)\n",
    "X_grid = X_grid.to(torch.float32)\n",
    "X_grid = X_grid.cuda()\n",
    "\n",
    "t = model(X_grid)\n",
    "out = F.softmax(t,dim=1)\n",
    "plt.contourf(X0grid,X1grid,out.cpu().detach().numpy()[:,1].reshape((101,101)),alpha=0.5)\n",
    "plt.scatter(X0[:,0],X0[:,1],c=y0)\n",
    "plt.scatter(X0_test[:,0],X0_test[:,1],c=y0_test,marker='x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2c $L_2$ Regularization for MNIST\n",
    "As we've seen before, image data gives us an opportunity to look at the weights.  Because $L_2$ regularization is manipulating weights directly, this gives us an opportunity to get a sense of *what qualitative effect the regularization is having on the weights*.  Let's apply this to Labeled Faces in the Wild, which we're now used to seeing. \n",
    "\n",
    "As usual, the data manipulation ritual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# In order to run this in class, we're going to reduce the dataset by a factor of 5\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, cache=True)\n",
    "X/=255.\n",
    "y = y.astype(int)\n",
    "X,X_test,y,y_test = train_test_split(X,y,test_size=10000)\n",
    "\n",
    "# Extract number of data points, and the height and width of the images for later reshaping\n",
    "m = X.shape[0]\n",
    "n = X.shape[1]\n",
    "\n",
    "h = 28\n",
    "w = 28\n",
    "\n",
    "N = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(X)\n",
    "X_test = torch.from_numpy(X_test)\n",
    "y = torch.from_numpy(y)\n",
    "y_test = torch.from_numpy(y_test)\n",
    "\n",
    "X = X.to(torch.float32)\n",
    "X_test = X_test.to(torch.float32)\n",
    "y = y.to(torch.long)\n",
    "y_test = y_test.to(torch.long)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "X = X.to(device)\n",
    "X_test = X_test.to(device)\n",
    "y = y.to(device)\n",
    "y_test = y_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "training_data = TensorDataset(X,y)\n",
    "test_data = TensorDataset(X_test,y_test)\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = torch.utils.data.DataLoader(dataset=training_data,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "batch_size = 256\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's construct a simple neural network, not dissimilar to the ones that you created "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        This method is where you'll want to instantiate parameters.\n",
    "        we do this by creating two linear transformation functions, l1 and l2, which \n",
    "        have encoded in it both the weight matrices W_1 and W_2, and the bias vectors\n",
    "        \"\"\"\n",
    "        super(Net,self).__init__()\n",
    "        self.l1 = nn.Linear(n,128) # Transform from input to hidden layer\n",
    "        self.l2 = nn.Linear(128,N)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        This method runs the feedforward neural network.  It takes a tensor of size m x 784,\n",
    "        applies a linear transformation, applies a sigmoidal activation, applies the second linear transform \n",
    "        and outputs the logits.\n",
    "        \"\"\"\n",
    "        a1 = self.l1(x)\n",
    "        z1 = torch.sigmoid(a1)   \n",
    "        a2 = self.l2(z1)\n",
    "        \n",
    "        return a2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's try training the model without L2 regularization.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
    "gamma=0\n",
    "#gamma = 2e5\n",
    "\n",
    "epochs = 30\n",
    "# Loop over the data\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    # Loop over each subset of data\n",
    "    for d,t in train_loader:\n",
    "\n",
    "        # Zero out the optimizer's gradient buffer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Make a prediction based on the model\n",
    "        outputs = model(d)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs,t)\n",
    "        for p in model.parameters():\n",
    "            if p.dim()>1:\n",
    "                loss += gamma/(2*d.shape[0])*(p**2).mean()\n",
    "        \n",
    "\n",
    "        # Use backpropagation to compute the derivative of the loss with respect to the parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Use the derivative information to update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "    model.eval()\n",
    "    # After each epoch, compute the test set accuracy\n",
    "    total=0.\n",
    "    correct=0.\n",
    "    # Loop over all the test examples and accumulate the number of correct results in each batch\n",
    "    for d,t in test_loader:\n",
    "        outputs = model(d)\n",
    "        _, predicted = torch.max(outputs.data,1)\n",
    "        total += float(t.size(0))\n",
    "        correct += float((predicted==t).sum())\n",
    "    total_train = 0\n",
    "    correct_train = 0\n",
    "    for d,t in train_loader:\n",
    "        outputs = model(d)\n",
    "        _, predicted = torch.max(outputs.data,1)\n",
    "        total_train += float(t.size(0))\n",
    "        correct_train += float((predicted==t).sum())\n",
    "        \n",
    "    # Print the epoch, the training loss, and the test set accuracy.\n",
    "    print(epoch,loss.item(),100.*correct_train/total_train,100.*correct/total)\n",
    "params = [p for p in model.l1.parameters()]\n",
    "W1_noreg = params[0].cpu().detach().numpy().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's try it with regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
    "gamma = 1e4\n",
    "\n",
    "epochs = 30\n",
    "# Loop over the data\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    # Loop over each subset of data\n",
    "    for d,t in train_loader:\n",
    "\n",
    "        # Zero out the optimizer's gradient buffer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Make a prediction based on the model\n",
    "        outputs = model(d)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs,t)\n",
    "        for p in model.parameters():\n",
    "            if p.dim()>1:\n",
    "                loss += gamma/(2*d.shape[0])*(p**2).mean()\n",
    "        \n",
    "\n",
    "        # Use backpropagation to compute the derivative of the loss with respect to the parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Use the derivative information to update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "    model.eval()\n",
    "    # After each epoch, compute the test set accuracy\n",
    "    total=0.\n",
    "    correct=0.\n",
    "    # Loop over all the test examples and accumulate the number of correct results in each batch\n",
    "    for d,t in test_loader:\n",
    "        outputs = model(d)\n",
    "        _, predicted = torch.max(outputs.data,1)\n",
    "        total += float(t.size(0))\n",
    "        correct += float((predicted==t).sum())\n",
    "    total_train = 0\n",
    "    correct_train = 0\n",
    "    for d,t in train_loader:\n",
    "        outputs = model(d)\n",
    "        _, predicted = torch.max(outputs.data,1)\n",
    "        total_train += float(t.size(0))\n",
    "        correct_train += float((predicted==t).sum())\n",
    "        \n",
    "    # Print the epoch, the training loss, and the test set accuracy.\n",
    "    print(epoch,loss.item(),100.*correct_train/total_train,100.*correct/total)\n",
    "params = [p for p in model.l1.parameters()]\n",
    "W1_L2 = params[0].cpu().detach().numpy().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the regularized model has both reduced training set accuracy *and* reduced test set accuracy.  This implies that our regularization is not very useful: that's because this dataset is, in fact, not very \"noisy\" (in the sense of multiple overlapping data points of different classes; image data is usually this way because it's so high dimensional).  However, if we plot some of the features being extracted in the form of extracted weight matrices, we can learn something about the effect of $L_2$ regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(nrows=2,ncols=6)\n",
    "fig.set_size_inches(10,4)\n",
    "for i in range(6):\n",
    "    axs[0,i].imshow(W1_noreg[:,np.random.randint(W1_noreg.shape[1])].reshape((h,w)))\n",
    "    axs[1,i].imshow(W1_L2[:,np.random.randint(W1_L2.shape[1])].reshape((h,w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because $L_2$ regularization penalizes large weights more than small ones, it has the tendency to ensure that all the weights are around the same size, which also means that it tends towards extracting larger, smoother features.\n",
    "\n",
    "## IC8B $L_1$ regularization\n",
    "We chose to regularize by penalizing the sum of squared weights (if you don't believe this, go back to the formula).  However, there are many other ways that we could exert pressure on the weights to behave one way or another.  One very interesting possibility is so-called $L_1$ regularization, which penalizes the $L_1$ norm of the weights.  What, you ask, is the $L_1$ norm?  It is the *sum of absolute values*:\n",
    "$$\n",
    "\\|W^{(l)}\\|_1 = \\sum_{i} \\sum_{j} |w_{ij}^{(l)}|.\n",
    "$$\n",
    "Superficially, it would seem that this would do the same thing as $L_2$ regularization (make the weights smaller), and it is true that it does have this property.  However, it has a very different effect on the *distribution* of weights.  **Implement $L_1$ regularization, using the code above as a starting point.  Discuss the qualitative effect that this form of regularization has on the resulting weight matrices.**  Hint 1: (You might have to do a little bit of searching to find weight matrices that aren't all close to zero).  Hint 3: (Take the derivative of the $L_1$ norm.  At each iteration of gradient descent, how much are large weights reduced versus small weights?)\n",
    "\n",
    "Finally, because there just aren't that many sensible norms in the world that we can penalize, there is a third form of direct regularization called *elastic net regularization*.  Despite the fancy name, it's just a combination of $L_1$ and $L_2$ regularization, each with their own weighting factor, which leads to the following objective function\n",
    "$$ \\mathcal{L}' = \\mathcal{L} + \\sum_{l=1}^L \\left[ \\frac{\\delta}{m} \\|W^{(l)}\\|_1 + \\frac{\\gamma}{2 m} \\|W^{(l)}\\|_2^2 \\right]. $$\n",
    "**Implement elastic weight regularization, and comment on the type of features produced**\n",
    "\n",
    "## 3 More exotic forms of regularization\n",
    "Direct manipulation of weights is perhaps the most obvious way of making the model parameters behave in the way that we would like them to, but they don't often help much on problems of fundamental interest.  Perhaps that's because there's a bit of a jump between \"Make the $L_1$ norm small\" and \"Don't overfit the data.\" More interesting mechanisms for regularization exist that force the model to perform the latter task in somewhat more direct and clever ways.\n",
    "\n",
    "## 3A Dropout\n",
    "If neural networks can (however tenuously) be thought of as an analog of an animal brain, then overfitting is reasonably well described as memorization.  For the student taking a test, instructors would prefer the student be able to answer a question in a constructive way by understanding the premise of the question being asked and thoughtfully constructing a response.  This is desirable because this mechanism *does not depend upon the specific way in which the question was asked*.  An alternative mechanism that less desirable students have been using forever is memorization: if the exact wording of the question (and its answer) are known beforehand, one can simply relate question to answer with no deeper intermediate analysis.  This latter is typically a great strategy for achieving training set accuracy, but very poor for test set accuracy.  \n",
    "\n",
    "In the context of neural networks, these models often have sufficient degrees of freedom to simply memorize inputs.  As an extreme example, if the number of hidden layer nodes is similar to the number of data points, then weights can be adjusted such that exactly one hidden layer node gets activated for exactly one training example, and that hidden layer nodes has non-zero weight to exactly one softmax input.  Thus we have a network that sees a particular pattern of pixels (that it's seen before), and knows that that particular pattern corresponds to (say) a seven.  *Critically, come test time, a correct classification is only possible if the input is (nearly) exactly the same as what the network memorized*.  The useful capacity to weigh the contributions of different features is lost.\n",
    "\n",
    "*Dropout* is a mechanism for ensuring that the above scenario does not occur.  It works as follows: for each training step, we select a random subset of nodes in a layer that we would like to apply dropout to (sometimes a hidden layer, sometimes an input layer, sometimes both), and *turn that node off*, i.e. set its output to zero.  This can be visualized with a classic plot from the [original paper by Srivastava et al.](http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer) way back in 2014.\n",
    "\n",
    "<img src=\"images/dropout.png\">\n",
    "Since the node is turned off, its forward mode output is equal to zero, which causes the gradient of any weights directly connected to it to be zero (**Can you show that this is true?**).  *Thus, the neural network is forced to learn to operate without only a random subset of its features available to it at any given time.* If only a random subset of features are available, then the network cannot rely on any given pattern, and thus it cannot simply memorize the training data! (A clever instructor might be able to develop questions that are similarly un-memorizable for their human pupils).  At test time, we don't drop nodes (that would be silly.  Why have them at all then?), but we do scale their outputs by the probability that they were dropped during the training process.  \n",
    "\n",
    "Then dropout is easy to apply in pytorch: all we need to do is to instantiate a dropout layer and apply it after our activation function.  This essentially picks a random subset of the nodes with proportion $p$, and zeros out their output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        This method is where you'll want to instantiate parameters.\n",
    "        we do this by creating two linear transformation functions, l1 and l2, which \n",
    "        have encoded in it both the weight matrices W_1 and W_2, and the bias vectors\n",
    "        \"\"\"\n",
    "        super(Net,self).__init__()\n",
    "        self.l1 = nn.Linear(784,128) # Transform from input to hidden layer\n",
    "        self.l2 = nn.Linear(128,10)\n",
    "        \n",
    "        # Instantiate dropout layers\n",
    "        self.dropout_1 = nn.Dropout(p=0.2)\n",
    "        self.dropout_2 = nn.Dropout(p=0.5)\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        This method runs the feedforward neural network.  It takes a tensor of size m x 784,\n",
    "        applies a linear transformation, applies a sigmoidal activation, applies the second linear transform \n",
    "        and outputs the logits.\n",
    "        \"\"\"\n",
    "        # Apply dropout to the input\n",
    "        xd = self.dropout_1(x)\n",
    "        a1 = self.l1(xd)\n",
    "        z1 = torch.relu(a1)\n",
    "        \n",
    "        # Apply dropout to the hidden layer\n",
    "        z1d = self.dropout_2(z1)\n",
    "        \n",
    "        a2 = self.l2(z1)\n",
    "\n",
    "        return a2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train the model using a similar loop as before.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "total_train = 0\n",
    "correct_train = 0\n",
    "# Loop over the data\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    # Loop over each subset of data\n",
    "    for d,t in train_loader:\n",
    "\n",
    "        # Zero out the optimizer's gradient buffer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Make a prediction based on the model\n",
    "        outputs = model(d)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs,t)      \n",
    "\n",
    "        # Use backpropagation to compute the derivative of the loss with respect to the parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Use the derivative information to update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data,1)\n",
    "        total_train += float(t.size(0))\n",
    "        correct_train += float((predicted==t).sum())\n",
    "        \n",
    "    model.eval()\n",
    "    # After each epoch, compute the test set accuracy\n",
    "    total=0.\n",
    "    correct=0.\n",
    "    # Loop over all the test examples and accumulate the number of correct results in each batch\n",
    "    for d,t in test_loader:\n",
    "        outputs = model(d)\n",
    "        _, predicted = torch.max(outputs.data,1)\n",
    "        total += float(t.size(0))\n",
    "        correct += float((predicted==t).sum())\n",
    "        \n",
    "    # Print the epoch, the training loss, and the test set accuracy.\n",
    "    print(epoch,loss.item(),100.*correct_train/total_train,100.*correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout has worked (in a way that $L_2$ or $L_1$ regularization clearly did not): we get close to 98.3% test set accuracy, a full 0.5% improvement over the unregularized case we saw before!  Clearly this improvement is marginal, but that last bit of accuracy is always the most challenging to achieve.\n",
    "\n",
    "## 3B Data Augmentation\n",
    "Dropout is a fantastic tool for encouraging neural networks to overfit, but it is still suboptimal when compared to the one and only true solution to overfitting: get more data.  This is the one method that always works to justify the inclusion of more complex and powerful models.  The problem of course is that it's often expensive and sometimes impossible.  For example, with the MNIST dataset, we can't just go collect new handwritten digits...  Or can we?\n",
    "\n",
    "Consider the following digit images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(nrows=1,ncols=2)\n",
    "axs[0].imshow(X[5].cpu().reshape((28,28)))\n",
    "\n",
    "# What is being done here?\n",
    "from scipy.ndimage import rotate\n",
    "axs[1].imshow(rotate(X[5].cpu().reshape((28,28)),25,order=1,reshape=False))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are both, of course, still the same digit, and that is immediately recognizable to your human visual processing system because somehow it's learned to be *rotationally invariant*.  However, we haven't explicitly taught our neural network that same property: all the rotational invariance it has learned is that which comes from the 60k training examples.  We can force it to learn that property by feeding the network training examples that we have explicitly rotated (probably randomly, and probably within a set range of viable rotations.  **What happens if we train on data that is randomly rotated in the full 360 degrees?**\n",
    "\n",
    "The above process is called *data augmentation*, and is typically done in an *on-line* capacity, which means that as we load mini-batches, we perform some random transformation on each data point, then pass it on to the network as usual.  **Besides rotation, what kinds of transformations can you imagine that we might do on image data?**  We can implement this in pytorch with a custom data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset,TensorDataset\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# In order to run this in class, we're going to reduce the dataset by a factor of 5\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, cache=True)\n",
    "X/=255.\n",
    "y = y.astype(int)\n",
    "X,X_test,y,y_test = train_test_split(X,y,test_size=10000)\n",
    "\n",
    "X = torch.from_numpy(X)\n",
    "X_test = torch.from_numpy(X_test)\n",
    "y = torch.from_numpy(y)\n",
    "y_test = torch.from_numpy(y_test)\n",
    "\n",
    "X = X.to(torch.float32)\n",
    "X_test = X_test.to(torch.float32)\n",
    "y = y.to(torch.long)\n",
    "y_test = y_test.to(torch.long)\n",
    "\n",
    "### NOTE THAT WE ARE NOT YET PLACING OBJECTS ON GPU ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a random transform (20 degree rotation, 3 pixel translation, \n",
    "# scaling between 90% and 110%, and a 10 degree shear over the x-axis) \n",
    "transform = transforms.RandomAffine(20,translate=(0.1,0.1),scale=(0.9,1.1),shear=10)#,transforms.RandomResizedCrop(224),transforms.RandomHorizontalFlip()]\n",
    "\n",
    "# Custom dataset object\n",
    "class CustomTensorDataset(Dataset):\n",
    "    \"\"\"TensorDataset with support of transforms.\n",
    "    \"\"\"\n",
    "    def __init__(self, tensors, transform=None):\n",
    "        assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)\n",
    "        self.tensors = tensors\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Define a __getitem__ method to be called by the \n",
    "        # mini-batch loader \n",
    "        \n",
    "        x = self.tensors[0][index]\n",
    "\n",
    "        if self.transform:\n",
    "            # reshape data, apply transform, reflatten\n",
    "            x = x.reshape((28,28))\n",
    "            x = transforms.ToPILImage()(x)\n",
    "            x = self.transform(x)\n",
    "            x = transforms.ToTensor()(x)\n",
    "            x = x.flatten()\n",
    "            #print(x)\n",
    "\n",
    "        y = self.tensors[1][index]\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tensors[0].size(0)\n",
    "\n",
    "training_data = CustomTensorDataset([X,y],transform=transform)\n",
    "test_data = CustomTensorDataset([X_test,y_test])\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = torch.utils.data.DataLoader(dataset=training_data,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True,num_workers=24)\n",
    "\n",
    "batch_size = 256\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=False,num_workers=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the type of digits our augmented data set includes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = next(iter(train_loader))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig,axs = plt.subplots(nrows=5,ncols=6)\n",
    "axs = axs.ravel()\n",
    "for i,ax in enumerate(axs):\n",
    "    ax.imshow(t[0][i].reshape((28,28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "n = 784\n",
    "N = 10\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        This method is where you'll want to instantiate parameters.\n",
    "        we do this by creating two linear transformation functions, l1 and l2, which \n",
    "        have encoded in it both the weight matrices W_1 and W_2, and the bias vectors\n",
    "        \"\"\"\n",
    "        super(Net,self).__init__()\n",
    "        self.l1 = nn.Linear(n,128) # Transform from input to hidden layer\n",
    "        self.l2 = nn.Linear(128,N)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        This method runs the feedforward neural network.  It takes a tensor of size m x 784,\n",
    "        applies a linear transformation, applies a sigmoidal activation, applies the second linear transform \n",
    "        and outputs the logits.\n",
    "        \"\"\"\n",
    "        a1 = self.l1(x)\n",
    "        z1 = torch.sigmoid(a1)\n",
    "        \n",
    "        a2 = self.l2(z1)\n",
    "\n",
    "        return a2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, this model takes alot longer to train, mostly due to the overhead of augmentation (there are new libraries that allow augmentation to be done quickly and in situ on the GPU in order to help deal with this bottleneck).  We'll load a pre-trained model to help us get to results more quickly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "#model = Net()\n",
    "#model.to(device)\n",
    "model = torch.load(open('trained_with_augmentation.p','rb'))\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
    "epochs = 50\n",
    "\n",
    "total_train = 0\n",
    "correct_train = 0\n",
    "# Loop over the data\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    # Loop over each subset of data\n",
    "    for d,t in train_loader:\n",
    "        # Move batches to GPU\n",
    "        d,t = d.to(device),t.to(device)\n",
    "\n",
    "        # Zero out the optimizer's gradient buffer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Make a prediction based on the model\n",
    "        outputs = model(d)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs,t)  \n",
    "\n",
    "        # Use backpropagation to compute the derivative of the loss with respect to the parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Use the derivative information to update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data,1)\n",
    "        total_train += float(t.size(0))\n",
    "        correct_train += float((predicted==t).sum())\n",
    "        \n",
    "    model.eval()\n",
    "    # After each epoch, compute the test set accuracy\n",
    "    total=0.\n",
    "    correct=0.\n",
    "    # Loop over all the test examples and accumulate the number of correct results in each batch\n",
    "    for d,t in test_loader:\n",
    "        d,t = d.to(device),t.to(device)\n",
    "        outputs = model(d)\n",
    "        _, predicted = torch.max(outputs.data,1)\n",
    "        total += float(t.size(0))\n",
    "        correct += float((predicted==t).sum())\n",
    "        \n",
    "    # Print the epoch, the training loss, and the test set accuracy.\n",
    "    print(epoch,loss.item(),100.*correct_train/total_train,100.*correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation has gotten us to test set accuracies in excess of 98.5%, nearly a full percentage point better than the unregularized data!  This is a remarkable improvement for just jiggling our dataset a bit.  However, it makes sense why it works: the model never trains on the same data point twice, making memorization quite impossible.  Additionally, this property leads to some very compelling looking weight images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [p for p in model.parameters()]\n",
    "W1 = params[0].cpu().detach().numpy().T\n",
    "fig,axs = plt.subplots(nrows=1,ncols=6)\n",
    "fig.set_size_inches(10,4)\n",
    "for i in range(6):\n",
    "    axs[i].imshow(W1[:,np.random.randint(W1.shape[1])].reshape((28,28)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No pixel can ever really be relied upon to produce a definitive categorization, so the model has instead learned to recognize broader patterns that are more easily recognizable.  Interestingly, if you know something about classical image processing or numerical partial differential equations, some of the images above will look familiar: asymmetrical patches of positive and negative values are associated with taking the gradient of an image, while a positive patch surrounded by negative patches are analogous to taking the laplacian (essentially the second derivative, or *curvature* of an image).  We've long known that these are nice filters to apply for extracting relevant features from an image; our neural network has just learned these filters on its own!  This notion of *explicitly learning filters* to be applied to images (or any structured data, as we will see) for feature extraction turns out to be a very powerful idea that gives rise to the basis for the current state of the art in image processing (and various tasks in bioinformatics and audio analysis), the *convolutional neural network*.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
